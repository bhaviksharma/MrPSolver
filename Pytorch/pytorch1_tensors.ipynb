{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/v43SlgBcZ5Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy vs.Torch\n",
    "\n",
    "Numpy arrays and pytorch tensors can be created in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0,1,5)\n",
    "t = torch.linspace(0,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(48).reshape(3,4,4)\n",
    "t = np.arange(48).reshape(3,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]],\n",
       "\n",
       "       [[16, 17, 18, 19],\n",
       "        [20, 21, 22, 23],\n",
       "        [24, 25, 26, 27],\n",
       "        [28, 29, 30, 31]],\n",
       "\n",
       "       [[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]],\n",
       "\n",
       "       [[16, 17, 18, 19],\n",
       "        [20, 21, 22, 23],\n",
       "        [24, 25, 26, 27],\n",
       "        [28, 29, 30, 31]],\n",
       "\n",
       "       [[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, they have the same broadcasting rules. In order to use pytorch (and even numpy) most efficiently, one needs to have a very strong grasp on the broadcasting rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Broadcasting rules\n",
    "\n",
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when\n",
    "\n",
    "1. they are equal, or\n",
    "2. one of them is 1\n",
    "\n",
    "Example: The following are compatible (both 6 dimensional)\n",
    "\n",
    "- Shape1: (1,6,4,1,7,2)\n",
    "- Shape2: (5,6,1,3,1,2)\n",
    "\n",
    "observe that either the dimensions are same or one of them is 1 between Shape1 and Shape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "(1, 5)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((6,5))\n",
    "b = np.arange(5).reshape((1,5))\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((6,5))\n",
    "b = torch.arange(5).reshape((1,5))\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same number of dimensions. If one of the arrays/tensors has less dimensions than the other\n",
    "\n",
    "Example: Scaling each other the color channels of an image by a different amount:<br>\n",
    "&emsp; &emsp; Image (3d array): 256 x 256 x 3 <br>\n",
    "&emsp; &emsp; Scale (1d array):             3 <br>\n",
    "&emsp; &emsp; Result (3d array):256 x 256 x 3 <br>\n",
    "\n",
    "Here it checks last dimension for the above mentioned rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5,1.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "Result = Scale * Image\n",
    "print(Result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4876, -1.2668,  0.9319],\n",
       "         [-0.2365, -0.7776,  1.7210],\n",
       "         [-1.2332,  0.4431, -1.4029],\n",
       "         ...,\n",
       "         [-0.3949,  1.2514,  0.8014],\n",
       "         [-0.3161,  2.7752, -1.9957],\n",
       "         [ 0.0271, -0.5577,  0.4232]],\n",
       "\n",
       "        [[-1.2474,  2.8289, -0.6902],\n",
       "         [-0.5624, -0.2220,  0.0614],\n",
       "         [-1.0093,  1.3674, -2.0899],\n",
       "         ...,\n",
       "         [-0.3086, -0.2575,  0.1295],\n",
       "         [ 0.9888, -2.3733,  0.9128],\n",
       "         [ 0.6741,  1.5198,  0.9000]],\n",
       "\n",
       "        [[ 0.6294, -0.5966, -0.5844],\n",
       "         [ 0.6573, -0.2683,  0.3732],\n",
       "         [ 0.6097, -2.4371, -1.5944],\n",
       "         ...,\n",
       "         [ 0.0132, -0.0130, -0.1795],\n",
       "         [ 0.4778,  0.7356,  0.9148],\n",
       "         [-0.1930, -0.1843, -0.3399]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.4235,  0.6058, -0.1965],\n",
       "         [-0.8552,  0.0714, -1.0518],\n",
       "         [ 0.8501,  0.2962, -0.7398],\n",
       "         ...,\n",
       "         [-0.6483,  0.4685, -0.4696],\n",
       "         [-0.4783,  1.6980,  1.3606],\n",
       "         [ 0.0914,  4.6713, -0.4690]],\n",
       "\n",
       "        [[ 0.3681, -2.2435,  0.1469],\n",
       "         [ 0.4557, -0.0692,  1.5253],\n",
       "         [-0.3158, -1.7607, -0.8844],\n",
       "         ...,\n",
       "         [-0.0664, -0.0725, -0.0285],\n",
       "         [-0.0053,  0.8525, -1.1254],\n",
       "         [ 0.0594,  1.4019, -0.1984]],\n",
       "\n",
       "        [[-0.5829,  2.0312,  2.3272],\n",
       "         [-0.4116,  1.2002,  0.4041],\n",
       "         [-0.7471, -0.0690, -0.6157],\n",
       "         ...,\n",
       "         [-0.3050,  0.5382, -1.8045],\n",
       "         [-0.8191,  2.0648, -1.3792],\n",
       "         [-0.9779, -0.9398, -0.2845]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: One has a an array of 2 images and wants to scale the color channels of each image by slightly different amount: <br>\n",
    "&emsp; &emsp; Images (4d array):   2 x 256 x 256 x 3 <br>\n",
    "&emsp; &emsp; Scales (4d array):   2 x 1 x 1 x 3 <br>\n",
    "&emsp; &emsp; Results (4d array):  2 x 256 x 256 x 3 <br>\n",
    "\n",
    "2 here signifies 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2,256,256,3))\n",
    "\n",
    "# Say we want 2 different scales for 2 images, then we would define like this.\n",
    "Scales = torch.tensor([0.5,1.5,1,1.5,1,0.5]).reshape((2,1,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5000, 1.5000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.5000, 1.0000, 0.5000]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = Images * Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6929,  1.4107,  0.8972],\n",
       "          [ 0.4677, -0.0986, -0.2567],\n",
       "          [-0.2831, -0.2370,  0.1464],\n",
       "          ...,\n",
       "          [ 0.2556,  2.5052,  1.5145],\n",
       "          [ 0.7128, -2.1129, -1.1398],\n",
       "          [ 0.3209,  1.6698, -0.5017]],\n",
       "\n",
       "         [[ 0.3063, -1.8100, -0.2835],\n",
       "          [-0.3262, -1.5199, -1.1615],\n",
       "          [-0.8247,  0.1935,  0.3085],\n",
       "          ...,\n",
       "          [ 0.2399, -0.4343,  2.0732],\n",
       "          [-0.2324,  1.3758,  0.8689],\n",
       "          [ 0.1035,  1.0893,  0.1744]],\n",
       "\n",
       "         [[-0.3944, -0.5098,  1.3413],\n",
       "          [ 0.0570,  2.9195,  0.8431],\n",
       "          [-0.2680,  3.8371, -1.2710],\n",
       "          ...,\n",
       "          [-0.6567,  0.0108, -0.3122],\n",
       "          [-0.5072,  0.7982,  1.5671],\n",
       "          [-0.8977, -1.1838, -0.2149]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2987,  0.9782, -0.9533],\n",
       "          [-0.6666, -2.0188,  0.0994],\n",
       "          [-0.1500, -1.0863,  0.2345],\n",
       "          ...,\n",
       "          [ 0.5902, -0.2546,  2.4463],\n",
       "          [-0.1256,  0.9782, -0.9762],\n",
       "          [-0.2759,  0.6176,  0.4908]],\n",
       "\n",
       "         [[-0.0730, -0.0835, -0.1899],\n",
       "          [-0.7410,  0.0194, -0.6402],\n",
       "          [ 0.1248,  0.7275,  1.2141],\n",
       "          ...,\n",
       "          [-0.2574,  0.1310,  1.1207],\n",
       "          [ 0.4740,  3.0801, -0.5454],\n",
       "          [-0.7077,  1.9257,  0.8614]],\n",
       "\n",
       "         [[ 0.2618, -1.0292, -1.3196],\n",
       "          [ 0.5396,  2.3041, -1.5963],\n",
       "          [-1.1056,  0.3620,  0.4795],\n",
       "          ...,\n",
       "          [ 0.0667, -0.1281,  0.8506],\n",
       "          [-0.6403,  3.5002, -0.2528],\n",
       "          [ 0.3255,  0.0395,  1.8502]]],\n",
       "\n",
       "\n",
       "        [[[-1.4941,  1.2274,  0.5505],\n",
       "          [ 0.5840, -1.2924, -0.1288],\n",
       "          [ 0.4145,  0.4237, -0.1786],\n",
       "          ...,\n",
       "          [-1.3950,  0.7101,  0.3140],\n",
       "          [ 0.7392, -0.4346,  0.2708],\n",
       "          [ 0.3676,  1.3928,  0.2724]],\n",
       "\n",
       "         [[-1.4894, -0.5855,  0.0919],\n",
       "          [ 0.2234, -0.2259, -0.3820],\n",
       "          [-1.3965, -1.9417,  0.7161],\n",
       "          ...,\n",
       "          [ 0.1957, -0.7425, -0.4619],\n",
       "          [ 0.4108, -1.7844,  0.0198],\n",
       "          [ 0.5767,  2.1291,  0.2313]],\n",
       "\n",
       "         [[ 2.9521,  1.4866,  0.3193],\n",
       "          [ 0.5470,  2.5383,  0.1253],\n",
       "          [ 1.7437, -1.0650,  0.0346],\n",
       "          ...,\n",
       "          [-0.9229, -0.1307,  0.9624],\n",
       "          [ 0.4762,  0.3435,  0.0803],\n",
       "          [ 1.6258, -0.8366, -0.6510]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.4338, -0.7175, -0.6362],\n",
       "          [ 0.3176, -0.2720, -0.0497],\n",
       "          [-0.7642, -0.3735,  0.2693],\n",
       "          ...,\n",
       "          [ 0.2080,  0.0832, -0.6409],\n",
       "          [ 1.6803,  1.3144,  0.0555],\n",
       "          [ 1.9168,  1.1212, -0.3247]],\n",
       "\n",
       "         [[-1.3948, -1.1750, -0.2769],\n",
       "          [-0.3432, -0.3401, -0.6512],\n",
       "          [ 0.3548,  0.2139, -0.1157],\n",
       "          ...,\n",
       "          [-0.2471,  0.7311, -0.1491],\n",
       "          [-1.1411, -0.7140, -0.7546],\n",
       "          [-1.5899, -0.9651, -0.1547]],\n",
       "\n",
       "         [[ 0.4898, -0.2702,  0.1102],\n",
       "          [ 1.2979,  1.3133, -0.4639],\n",
       "          [ 0.8245, -0.7103,  0.2056],\n",
       "          ...,\n",
       "          [ 0.0279,  1.0388,  0.4218],\n",
       "          [-0.7798, -0.1700, -0.0187],\n",
       "          [-1.0167, -0.3437, -0.3306]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations Across Dimensions\n",
    "\n",
    "Of course simple operations can be done on 1D tensors just like on 1D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.3750), tensor(1.1087), tensor(3.), tensor(0.5000))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5,1,1,3])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But suppose we have 2D tensor and we want to compute the mean value of each columns:\n",
    "\n",
    "- Note: taking the mean of each column means taking the mean across the rows (which are the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.5000,  8.5000,  9.5000, 10.5000, 11.5000], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype = float).reshape((4,5))\n",
    "torch.mean(t,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done for higher dimensions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5,256,256,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the mean across the batch (i.e. size of the image batch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0642,  0.2051,  0.0551],\n",
       "         [ 0.7510, -0.6714,  0.1091],\n",
       "         [-0.5724,  0.2458, -0.3806],\n",
       "         ...,\n",
       "         [ 0.2940, -0.1770, -0.2411],\n",
       "         [ 0.8691, -0.4686, -0.1377],\n",
       "         [ 0.0602, -0.4241,  0.0614]],\n",
       "\n",
       "        [[-0.4278,  0.2958, -0.9688],\n",
       "         [-0.7137,  0.1241, -0.2354],\n",
       "         [-0.1568, -0.4136,  0.5662],\n",
       "         ...,\n",
       "         [ 0.7637,  0.5564,  0.4509],\n",
       "         [-0.9906, -0.6080, -0.2802],\n",
       "         [-0.0775, -0.8193, -0.2493]],\n",
       "\n",
       "        [[-0.4896,  0.0719,  0.9491],\n",
       "         [-0.2043, -0.8947,  0.7929],\n",
       "         [-0.0464, -0.0614,  0.5344],\n",
       "         ...,\n",
       "         [ 0.0168,  0.2275,  0.3948],\n",
       "         [-0.6240,  0.2267,  0.4433],\n",
       "         [ 0.0861,  0.2235,  0.1683]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0317,  0.2783,  0.8129],\n",
       "         [ 0.3143, -0.7961,  0.0895],\n",
       "         [-0.0102,  0.2944, -0.2746],\n",
       "         ...,\n",
       "         [-1.0010,  0.5071, -0.3565],\n",
       "         [-1.0485,  0.1542,  0.2210],\n",
       "         [ 0.2217,  0.0341,  0.6168]],\n",
       "\n",
       "        [[ 0.4715, -0.2121, -0.4676],\n",
       "         [-0.4901,  0.7432, -0.3903],\n",
       "         [ 0.5895,  0.5215, -0.2746],\n",
       "         ...,\n",
       "         [ 0.3496,  0.2054, -0.2271],\n",
       "         [-0.4660, -0.1835,  0.3821],\n",
       "         [-0.1238,  0.4933,  0.4429]],\n",
       "\n",
       "        [[-0.1794, -0.4850,  0.8279],\n",
       "         [-0.5792, -0.4249,  0.2257],\n",
       "         [ 0.4114,  0.9383, -0.2554],\n",
       "         ...,\n",
       "         [ 0.6974, -0.6293, -0.1283],\n",
       "         [ 0.9152, -0.3536,  0.5745],\n",
       "         [-0.3453,  0.3229,  0.5334]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0642,  0.2051,  0.0551],\n",
       "         [ 0.7510, -0.6714,  0.1091],\n",
       "         [-0.5724,  0.2458, -0.3806],\n",
       "         ...,\n",
       "         [ 0.2940, -0.1770, -0.2411],\n",
       "         [ 0.8691, -0.4686, -0.1377],\n",
       "         [ 0.0602, -0.4241,  0.0614]],\n",
       "\n",
       "        [[-0.4278,  0.2958, -0.9688],\n",
       "         [-0.7137,  0.1241, -0.2354],\n",
       "         [-0.1568, -0.4136,  0.5662],\n",
       "         ...,\n",
       "         [ 0.7637,  0.5564,  0.4509],\n",
       "         [-0.9906, -0.6080, -0.2802],\n",
       "         [-0.0775, -0.8193, -0.2493]],\n",
       "\n",
       "        [[-0.4896,  0.0719,  0.9491],\n",
       "         [-0.2043, -0.8947,  0.7929],\n",
       "         [-0.0464, -0.0614,  0.5344],\n",
       "         ...,\n",
       "         [ 0.0168,  0.2275,  0.3948],\n",
       "         [-0.6240,  0.2267,  0.4433],\n",
       "         [ 0.0861,  0.2235,  0.1683]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0317,  0.2783,  0.8129],\n",
       "         [ 0.3143, -0.7961,  0.0895],\n",
       "         [-0.0102,  0.2944, -0.2746],\n",
       "         ...,\n",
       "         [-1.0010,  0.5071, -0.3565],\n",
       "         [-1.0485,  0.1542,  0.2210],\n",
       "         [ 0.2217,  0.0341,  0.6168]],\n",
       "\n",
       "        [[ 0.4715, -0.2121, -0.4676],\n",
       "         [-0.4901,  0.7432, -0.3903],\n",
       "         [ 0.5895,  0.5215, -0.2746],\n",
       "         ...,\n",
       "         [ 0.3496,  0.2054, -0.2271],\n",
       "         [-0.4660, -0.1835,  0.3821],\n",
       "         [-0.1238,  0.4933,  0.4429]],\n",
       "\n",
       "        [[-0.1794, -0.4850,  0.8279],\n",
       "         [-0.5792, -0.4249,  0.2257],\n",
       "         [ 0.4114,  0.9383, -0.2554],\n",
       "         ...,\n",
       "         [ 0.6974, -0.6293, -0.1283],\n",
       "         [ 0.9152, -0.3536,  0.5745],\n",
       "         [-0.3453,  0.3229,  0.5334]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This means taking mean of each color channel across all the images.\n",
    "torch.mean(t,axis=0) # axis=0 ==> axis = -4 in this case (reverse indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the mean across the color channels (i.e. avg of red, blue and green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1617,  0.3292,  0.0342,  ...,  0.6304,  0.4107,  0.1644],\n",
       "         [ 0.4738, -0.2491, -0.3601,  ...,  1.7603, -1.2306, -1.1799],\n",
       "         [ 0.2525,  0.8443,  0.3030,  ..., -0.5203, -0.4542,  0.0079],\n",
       "         ...,\n",
       "         [ 1.0589, -0.0463, -0.6647,  ..., -0.6272, -0.8201, -0.3166],\n",
       "         [-0.5265,  0.0766,  0.7740,  ...,  0.5703, -1.0215,  0.7784],\n",
       "         [-0.3792,  0.0615,  0.4251,  ...,  0.8647, -0.6191,  0.6757]],\n",
       "\n",
       "        [[ 0.0559,  0.2359, -0.5436,  ...,  0.5156, -0.6520, -0.0832],\n",
       "         [-0.0943, -0.9542,  0.4693,  ..., -0.1839,  0.0130, -0.4686],\n",
       "         [ 0.1807, -0.5179, -0.2068,  ..., -0.1632, -0.0118,  0.2821],\n",
       "         ...,\n",
       "         [-0.1721, -0.4227,  0.2760,  ..., -0.0801,  0.0842,  0.1807],\n",
       "         [-0.7419, -0.0790,  0.4206,  ...,  0.1439,  0.6987, -0.1322],\n",
       "         [ 0.1527,  0.0508, -0.0990,  ..., -0.4904,  0.6641, -0.2374]],\n",
       "\n",
       "        [[ 0.2655,  0.4166, -0.5444,  ..., -0.9541, -0.1425,  0.2583],\n",
       "         [-0.2766, -0.4614,  0.6729,  ...,  0.2320, -1.4687,  0.2789],\n",
       "         [ 0.4041, -0.3674,  0.1105,  ...,  0.9815,  0.5528,  0.4605],\n",
       "         ...,\n",
       "         [ 0.0276,  1.1055,  0.0464,  ...,  0.1611,  0.1220,  0.0215],\n",
       "         [ 0.1251,  0.9191,  0.7481,  ...,  0.2579, -0.0166,  1.3497],\n",
       "         [ 0.3553,  0.8586,  0.1915,  ..., -1.2781,  0.8208, -0.2457]],\n",
       "\n",
       "        [[ 0.6579, -0.5428,  0.2710,  ..., -0.0321, -0.0086, -0.7621],\n",
       "         [-1.0808, -0.3001, -0.8175,  ...,  0.7254, -0.3485,  0.4239],\n",
       "         [ 0.4477,  0.1223,  0.3024,  ...,  0.6587, -0.2990,  0.1303],\n",
       "         ...,\n",
       "         [ 0.3529, -0.3184, -0.1807,  ..., -0.6570, -0.7864,  1.0410],\n",
       "         [ 0.1526, -0.6000,  0.4072,  ..., -0.2340, -0.3342, -0.2245],\n",
       "         [ 0.0933, -0.6983, -0.1352,  ...,  0.8299,  0.2804,  0.8306]],\n",
       "\n",
       "        [[-0.2769, -0.1244, -0.3959,  ..., -0.3665,  0.8303, -0.0815],\n",
       "         [-0.8567,  0.5899,  0.0283,  ...,  0.4177, -0.0966, -0.9646],\n",
       "         [-0.3994, -0.5915,  0.2019,  ...,  0.1084,  0.2889, -0.0843],\n",
       "         ...,\n",
       "         [ 0.4984, -0.9721,  0.5391,  ..., -0.2140,  0.2782,  0.5280],\n",
       "         [ 0.6437, -0.5454, -0.9558,  ..., -0.1915,  0.2280, -0.4173],\n",
       "         [ 0.0502, -1.5699,  1.4412,  ..., -0.0265,  0.7474, -0.1716]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=-1) #reverese indexing, axis = -1 ==> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=-1).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values (and get the corresponding indices):\n",
    "- This is done all the time in image segmentation models (i.e. take an image, decide which pixels correspond to, say, a car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t, axis=-1) #torch.max() returns 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5988e-01,  7.1235e-01,  7.2779e-01,  ...,  1.0628e+00,\n",
       "           2.2426e+00,  1.5064e+00],\n",
       "         [ 2.2642e+00,  5.1064e-01,  3.6486e-01,  ...,  2.3695e+00,\n",
       "          -8.0365e-01, -2.0256e-01],\n",
       "         [ 1.4702e+00,  1.4896e+00,  8.5115e-01,  ..., -1.4374e-01,\n",
       "           1.5337e-01,  1.0135e+00],\n",
       "         ...,\n",
       "         [ 1.3137e+00,  2.5316e-01, -4.0980e-01,  ...,  4.7765e-01,\n",
       "           4.3091e-01,  4.7359e-01],\n",
       "         [-4.3098e-01,  1.7360e+00,  1.7577e+00,  ...,  1.0432e+00,\n",
       "           3.5823e-01,  1.2910e+00],\n",
       "         [ 1.0890e+00,  2.6363e+00,  2.1503e+00,  ...,  1.5021e+00,\n",
       "           1.9044e+00,  1.9849e+00]],\n",
       "\n",
       "        [[ 8.5424e-01,  1.9954e+00,  3.6158e-01,  ...,  2.9042e+00,\n",
       "           7.0042e-01,  4.7188e-01],\n",
       "         [ 1.7775e+00,  1.9370e-01,  2.8727e+00,  ...,  8.5829e-01,\n",
       "           4.1965e-01,  4.4634e-02],\n",
       "         [ 1.9483e+00,  7.4862e-01,  3.4150e-02,  ...,  7.7793e-01,\n",
       "           5.1307e-01,  1.2506e+00],\n",
       "         ...,\n",
       "         [ 1.0090e-01,  5.0912e-01,  1.0249e+00,  ...,  1.9868e-01,\n",
       "           4.1222e-01,  5.6220e-01],\n",
       "         [-6.0357e-01,  7.1464e-01,  6.0797e-01,  ...,  6.1417e-01,\n",
       "           1.3137e+00,  1.3572e+00],\n",
       "         [ 7.2582e-01,  6.7339e-01,  1.8622e-01,  ...,  2.0701e+00,\n",
       "           1.3744e+00,  1.1088e+00]],\n",
       "\n",
       "        [[ 6.4280e-01,  1.4756e+00, -1.6696e-01,  ..., -1.7557e-01,\n",
       "           3.7234e-01,  1.6477e+00],\n",
       "         [ 1.1579e+00, -1.0230e-01,  1.4758e+00,  ...,  9.6672e-01,\n",
       "          -5.1797e-01,  1.0585e+00],\n",
       "         [ 1.1784e+00,  8.8815e-01,  1.3320e+00,  ...,  1.4358e+00,\n",
       "           1.6400e+00,  1.7374e+00],\n",
       "         ...,\n",
       "         [ 2.1009e-01,  1.9154e+00,  8.2938e-01,  ...,  1.0540e+00,\n",
       "           9.4116e-01,  1.3745e+00],\n",
       "         [ 2.8209e-01,  1.4939e+00,  1.4542e+00,  ...,  1.5572e+00,\n",
       "           1.2260e+00,  1.6676e+00],\n",
       "         [ 8.3080e-01,  1.2358e+00,  8.4698e-01,  ..., -9.5144e-01,\n",
       "           2.1855e+00, -1.6865e-01]],\n",
       "\n",
       "        [[ 8.0195e-01, -4.2576e-01,  9.1020e-01,  ...,  5.9999e-01,\n",
       "           9.6182e-01,  2.6929e-01],\n",
       "         [ 1.8566e-02,  2.0544e-01,  4.2094e-01,  ...,  2.1204e+00,\n",
       "           1.8229e+00,  1.7418e+00],\n",
       "         [ 1.2557e+00,  1.6850e+00,  1.4470e+00,  ...,  1.0192e+00,\n",
       "           3.3454e-01,  9.5255e-01],\n",
       "         ...,\n",
       "         [ 9.8030e-01,  4.3366e-01,  1.3471e+00,  ...,  8.2967e-01,\n",
       "           1.3638e-01,  1.4443e+00],\n",
       "         [ 1.6855e+00,  2.9283e-03,  8.8583e-01,  ..., -1.5505e-01,\n",
       "           4.9164e-01,  5.0152e-01],\n",
       "         [ 1.4558e+00,  1.4419e-01,  3.3334e-01,  ...,  1.8540e+00,\n",
       "           1.4895e+00,  1.1488e+00]],\n",
       "\n",
       "        [[ 1.2664e-01,  1.2336e+00,  1.3598e+00,  ...,  4.4197e-01,\n",
       "           9.4738e-01,  3.0737e-01],\n",
       "         [-2.6665e-01,  2.0267e+00,  6.7933e-01,  ...,  1.2055e+00,\n",
       "           1.1090e+00, -4.5063e-01],\n",
       "         [ 7.7477e-01,  7.7495e-01,  5.8220e-01,  ...,  1.7296e+00,\n",
       "           2.9976e+00,  8.0776e-01],\n",
       "         ...,\n",
       "         [ 1.7230e+00, -1.4241e-01,  1.2059e+00,  ...,  5.5640e-01,\n",
       "           5.6752e-01,  1.7194e+00],\n",
       "         [ 1.6171e+00,  9.3582e-01, -1.7732e-01,  ...,  2.0900e+00,\n",
       "           1.1038e+00, -6.7862e-02],\n",
       "         [ 6.9869e-01, -1.3619e+00,  1.8692e+00,  ...,  9.7726e-01,\n",
       "           1.0668e+00,  5.4509e-01]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1,  ..., 0, 0, 2],\n",
       "         [1, 1, 2,  ..., 0, 1, 2],\n",
       "         [2, 2, 1,  ..., 2, 2, 0],\n",
       "         ...,\n",
       "         [1, 2, 1,  ..., 1, 2, 2],\n",
       "         [1, 1, 0,  ..., 2, 2, 1],\n",
       "         [2, 2, 1,  ..., 0, 0, 2]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 1, 0, 2],\n",
       "         [0, 2, 2,  ..., 0, 1, 2],\n",
       "         [2, 2, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 0, 2,  ..., 1, 2, 1],\n",
       "         [2, 1, 2,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 0, 0, 2]],\n",
       "\n",
       "        [[0, 0, 2,  ..., 0, 2, 0],\n",
       "         [1, 1, 2,  ..., 1, 1, 2],\n",
       "         [1, 2, 2,  ..., 1, 1, 2],\n",
       "         ...,\n",
       "         [2, 0, 0,  ..., 2, 1, 2],\n",
       "         [0, 2, 1,  ..., 0, 2, 1],\n",
       "         [1, 1, 1,  ..., 0, 2, 1]],\n",
       "\n",
       "        [[2, 2, 1,  ..., 2, 0, 1],\n",
       "         [1, 1, 1,  ..., 1, 2, 0],\n",
       "         [2, 2, 2,  ..., 0, 1, 1],\n",
       "         ...,\n",
       "         [2, 0, 1,  ..., 1, 1, 2],\n",
       "         [0, 1, 1,  ..., 0, 2, 0],\n",
       "         [2, 2, 1,  ..., 1, 2, 2]],\n",
       "\n",
       "        [[2, 2, 1,  ..., 0, 0, 0],\n",
       "         [2, 1, 2,  ..., 0, 0, 0],\n",
       "         [1, 0, 1,  ..., 2, 2, 1],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 1, 2, 0],\n",
       "         [0, 1, 0,  ..., 1, 1, 2],\n",
       "         [2, 1, 0,  ..., 0, 2, 1]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices # 0,1,2 means red, green, blue etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 256, 256]), torch.Size([5, 256, 256]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape, indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So where do Pytorch and Numpy differ?\n",
    "\n",
    "Pytorch starts to really differ from numpy in terms of automatically computing gradients of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\sum_i x_i^{3}$$\n",
    "has a gradient <br>\n",
    "$$ \\frac{\\partial y}{\\partial x} = 3x_i^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 4.],\n",
       "        [3., 2.]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[5,4], [3,2]],dtype=float, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(224., dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75., 48.],\n",
       "        [27., 12.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() #compute gradients\n",
    "x.grad #print the gradient (everything that has happed to x, hence gradient of x stored in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[75., 48.],\n",
       "        [27., 12.]], dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x.pow(2) #this matches with above result, hence pytorch has calculated gradients correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic computation of gradients is the backbone of training deep learning models. Unlike in the example above, most gradient computations don't have an analytical formula, so the automatic computation of gradients is essential. In general, if one has\n",
    "\n",
    "$$y = f(\\vec x)$$\n",
    "\n",
    "then pytorch can compute ${\\partial y} / {\\partial x}$\n",
    ". For each of element of the vector \n",
    ". In the context of machine learning, \n",
    " contains all the weights (also known as parameters) of the neural network and  is the Loss Function of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Benefits\n",
    "\n",
    "**In addition, any sort of large matrix multiplication problem is faster with torch tensors than it is with numpy arrays, especially if you're running on a GPU**\n",
    "\n",
    "Using torch: faster with CPU\n",
    "Using torch: Much much faster with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((1000,1000))\n",
    "B = torch.randn((1000,1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014809000014793128\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A,B)\n",
    "t2 = time.perf_counter()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(int(1e6)).reshape((1000,1000))\n",
    "B = np.random.randn(int(1e6)).reshape((1000,1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059518199996091425\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is more than 4 times faster here. But this comes at the expense of memory (RAM) as tensor objects are heavier than numpy arrays."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch_Mr-P-Solver')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd333f409411bc5446bc89fd81b942417c0fcf2aa15c46865d6096bbe4424b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
