{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/w9U57o6wto0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose one has data that consists of an independent vector $x_i$ and a dependent vector $y_i$  and  ($i$ is the ith value in the data set). For example:\n",
    "\n",
    " - $x_i$ is the height of the $i$ th person, and $y_i$ is their weight (predict weight using height)\n",
    "- $x_i$ is a picture of a handwritten digit, and $y_i$ is the digit itself (predict numbers from hand written numbers)\n",
    "- $x_i$ is a CT scan of a patient, and $y_i$ are the corresponding pixels corresponding to tumours (my research)\n",
    "<br>\n",
    "<br>\n",
    "The goal of a neural network is as follows. Define a function $f$ that depends on parameters $a$ that makes predictions\n",
    "$$\\hat{y_i} = f(x_i ; a)$$\n",
    "\n",
    "One wants to make $\\hat{y_i}$ (the predictions) and $y_i$ (the true values) as close as possible by modifying the values of $a$. What does as close as possible mean? This depends on the task. In general, one defines a similarity function (or Loss function) $L(y, \\hat{y})$. The more similar all the $y$ s and $\\hat{y_i}$ s are, the smaller $L$ should be. For example 1 above, this could be as simple as\n",
    "$$L(y, \\hat{y}) = \\sum_i{(y_i - \\hat{y_i})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "y = torch.tensor([1,5,2,5]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 2.],\n",
       "        [5., 2.],\n",
       "        [1., 3.],\n",
       "        [7., 6.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So $x_1$ = (6,2) , $x_2$ = (5,2) , ...\n",
    "- So $y_1$ = 1, $y_2$ = 5 , ...\n",
    "\n",
    "We want to find a function $f$ that depends on parameters $a$ that gives $y$ for $x$ .\n",
    "<br>\n",
    "<br>\n",
    "IDEA:<br>\n",
    "These are basically weight matrices:\n",
    "1. First multiply each element in $x$ by a 8 x 2 matrix (this is 16 parameters $a_i$) -- FIRST LAYER\n",
    "- *In my understanding this should be 2 x 8 matrix* - but I am wrong, don't know why\n",
    "2. Then multiply each element in $x$ by a 1 x 8 matrix (this is 8 parameters $a_i$) -- SECOND LAYER\n",
    "\n",
    "Define a matrix (takes in a 2d vector and returns a 8d vector).\n",
    "\n",
    "- **IMPORTANT**: When the matrix is created, it is initially created with random values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=8, bias=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = nn.Linear(2, 8, bias=False) #take in 2 features and outputs 8 features.\n",
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1619, -0.7091,  0.1967,  1.9435, -3.9459, -4.8398,  3.0988, -4.1165],\n",
       "        [ 3.6198, -0.4581,  0.0821,  1.5131, -3.2544, -4.2518,  2.8031, -3.5238],\n",
       "        [ 1.9062,  0.9450, -0.6216, -0.5278, -0.3873, -2.5560,  2.2826, -1.4334],\n",
       "        [ 6.5228,  0.6346, -0.6703,  1.0963, -4.2319, -8.0518,  6.0437, -5.8301]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain this with a second matrix M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=1, bias=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2 = nn.Linear(8, 1, bias=False)\n",
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2415],\n",
       "        [-1.9139],\n",
       "        [-0.7412],\n",
       "        [-3.1205]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** M2(M1(x)) is 2D but y is 1D. So we want our prediction in 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2415, -1.9139, -0.7412, -3.1205], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M2(M1(x)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of the matrices M1 and M2 consitute the weights $a$ of the network defined above. In order to optimize for these weights, we first construct our network $f$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the network using a subclass of the nn.Module allows the parameters of the network to be conveniently stored. This will be useful later when we need to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = MyNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9743, 0.8129, 0.1700, 1.1472], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5724,  0.1172],\n",
      "        [ 0.6514, -0.2345],\n",
      "        [-0.1642, -0.3892],\n",
      "        [ 0.6756,  0.3269],\n",
      "        [-0.2448, -0.3653],\n",
      "        [-0.1739,  0.1717],\n",
      "        [ 0.6896, -0.4338],\n",
      "        [ 0.3598, -0.1451]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0944, -0.2798, -0.1542,  0.2334,  0.2602, -0.0479,  0.2668, -0.0611]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for par in f.parameters(): #parameter() is an inbuilt method of nn.module to store the weights\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "for par in f.parameters():\n",
    "    print(par.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9743, 0.8129, 0.1700, 1.1472], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = f(x)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust $a$ so that $y$ and $\\hat{y}$ are similar\n",
    "\n",
    "Now we define the loss function $L$, which provides a metric of similarity between $y$ and $\\hat{y}$. In this case, we will use the mean squared error loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9314, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = nn.MSELoss()\n",
    "L(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming whether it is doing the mean-sqaured error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9314, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((y-yhat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $L$ depends on $a$ , since our predictions $\\hat{y}$ depend on the parameters of the network $a$ . In this sense, $L = f(a)$. **The main idea behind machine learning** is to compute\n",
    " $$\\frac{\\partial{L}}{\\partial{a_i}}$$\n",
    "\n",
    " for each parameter $a_i$ of the network. Then we adjust each parameter as follows:\n",
    " $$ a_i \\rightarrow a_i - \\ell \\frac{\\partial{L}}{\\partial{a_i}}$$\n",
    "\n",
    " where $ \\ell $ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function that only depends on 1 paramter:\n",
    "\n",
    "<img src=\"https://github.com/lukepolson/youtube_channel/raw/404c306bf6976c12f1bb2afba709294f9fe3d3fc/images/loss.PNG\" style=\"width:1000px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to do this over and over again, until one reaches a minimum for $L$ . **This is called gradient descent**.\n",
    "- Each pass of the full data set $x$ is called an **epoch**. In this case, we are evaluating $\\frac{\\partial{L}}{\\partial{a_i}}$ on the entire dataset $x$ each time we iterate $ a_i \\rightarrow a_i - \\ell \\frac{\\partial{L}}{\\partial{a_i}}$, so each iteration corresponds to an epoch.\n",
    "- For a very large dataset, the data is divided into batches. Say we have 5 batches, then $\\frac{\\partial{L}}{\\partial{a_i}}$ is calculated for each batch separately. When all 5 batches are done, that's called 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD(meaning stochastic gradient descent) takes in all model parameters $a$ along with the learning rate $ \\ell $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(f.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the parameters over and over to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    opt.zero_grad() #flush previous epoch's gradient\n",
    "    loss_value = L(y,f(x)) #compute loss\n",
    "    loss_value.backward() #compute gradient\n",
    "    opt.step() #perform iteration using gradient above\n",
    "    losses.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3727, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3726694583892822"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_value.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot $ L(a)$  as a function of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFzCAYAAAD47+rLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgKUlEQVR4nO3de5SlVX3m8ec559S17w3VCt00zS0qoFysEBUhKEZRvKBjDEZMxnENScZRdGaNlxVnaVyZWcnoqJmMEAnGECAYR2B0EQchBu8CFhehm4vQXLu5dEF309fqrstv/njfqi6b6qZO1anz9mZ/Pytnvee857J37YAPe7/73dsRIQAAkIZa1RUAAADTR3ADAJAQghsAgIQQ3AAAJITgBgAgIQQ3AAAJaVRdgek4+OCDY9WqVVVXAwCAtrj11lufjoi+qd5LIrhXrVqlgYGBqqsBAEBb2H5kX+8xVA4AQEIIbgAAEkJwAwCQEIIbAICEENwAACSE4AYAICGVBLftC2yvtr3G9kerqAMAAClqe3DbPl7Sv5d0iqQTJL3V9jHtrgcAACmqosf9Mkk3RcSOiBiR9ENJ76ygHgAAJKeK4F4t6XTbB9nulfQWSYft/SHb59sesD0wODjY9koCAHAgantwR8Q9kv5S0g2SrpP0S0kjU3zu4ojoj4j+vr4pl2sFACA7lUxOi4ivRcTJEXG6pI2S7m9X2Xc/vkU/X/tMu4oDAKClqppVvqw8rpT0LklXtqvsv/3xg/rEVXe2qzgAAFqqqt3BrrJ9kKRhSR+KiE3tKtiSxiLaVRwAAC1VSXBHxGlVlCtJtkVuAwBSld3KaTVLQXIDABKVYXBbY+Q2ACBR2QW3zTVuAEC6Mgxui9gGAKQqu+DmGjcAIGXZBXcxVF51LQAAmJnsgrtm0+MGACQry+Cmxw0ASFV2wc2scgBAyvILbrFyGgAgXdkFN7PKAQApyy+4a1zjBgCkK7vgZncwAEDK8gtuVk4DACQsu+DmGjcAIGXZBTcrpwEAUpZdcLNyGgAgZdkFt1k5DQCQsOyCu+biSK8bAJCi7ILbKpKbXjcAIEXZBTc9bgBAyvIL7ho9bgBAurIL7nGsngYASFF2wV2zq64CAAAzlmFwF0d63ACAFGUY3FzjBgCkK7vgNj1uAEDCMgzuIrnJbQBAirILbu7jBgCkLLvgHp9TzjVuAECKsgvu8QVY6HEDAFKUXXCbWeUAgIRlF9xc4wYApCy74GZ3MABAyrIL7oket0huAEB6MgxuetwAgHRVEty2P2Z7je3Vtq+03d2+wovDGMkNAEhQ24Pb9nJJH5HUHxHHS6pLOrdd5bM7GAAgZVUNlTck9dhuSOqV9Hi7CmZ3MABAytoe3BGxXtIXJD0q6QlJz0bE9e0qn2vcAICUVTFUvkTSOyQdIelQSfNsnzfF5863PWB7YHBwsIXlF0d63ACAFFUxVP4GSQ9FxGBEDEu6WtJr9v5QRFwcEf0R0d/X19eywtkdDACQsiqC+1FJr7Ld6yJFz5R0T7sKZ+U0AEDKqrjGfbOkb0m6TdJdZR0ublf5rJwGAEhZo4pCI+Izkj5TRdmsnAYASFl2K6dN7A42VnFFAACYgQyDuzgyqxwAkKLsgrvGrHIAQMIyDO7iyDVuAECKMgxuZpUDANKVXXCLa9wAgIRlF9xc4wYApCzD4C6OrJwGAEhRdsHNymkAgJRlF9z0uAEAKcsuuM2scgBAwrILbnrcAICUZRfc9LgBACnLLrhZOQ0AkLLsgpseNwAgZRkGd3Fk5TQAQIqyC+7xldMYKQcApCjD4C6O9LgBACnKMLi5xg0ASFd2wT2OHjcAIEXZBTe7gwEAUpZfcJd/MSunAQBSlF1wszsYACBl2QU3K6cBAFKWXXCzchoAIGXZBTe7gwEAUpZdcO/pcRPcAID0ZBfce3rc1dYDAICZyDC4ucYNAEhXdsE9jqFyAECKsgvuWo3dwQAA6covuNkdDACQsOyCm5XTAAApyy646XEDAFKWXXCP38dNbAMAUpRdcLNyGgAgZW0PbtsvsX3HpMcW2x9tY/mSpDEucgMAEtRod4ERcZ+kEyXJdl3SeknXtKt87gYDAKSs6qHyMyWtjYhH2lUgu4MBAFJWdXCfK+nKdhZornEDABJWWXDb7pT0dkn/Zx/vn297wPbA4OBgy8odX6uc3AYApKjKHvebJd0WEU9N9WZEXBwR/RHR39fX17JCuY8bAJCyKoP7vWrzMLnE7mAAgLRVEty2eyX9jqSrqyhfoscNAEhT228Hk6SI2CHpoCrKHu9xAwCQoqpnlbfdxDVuxsoBAAnKLri5jxsAkLLsgnvPymkkNwAgPdkFNz1uAEDKsgtuqeh1s3IaACBFWQa3bW4HAwAkKcvgLnrcVdcCAIDmZRncRY+76loAANC8PINbXOMGAKQpy+Cu2dwMBgBIUqbBzcppAIA0ZRrcXOMGAKQpy+CW2R0MAJCmLIObHcIAAKnKNLjpcQMA0pRlcLNyGgAgVVkGNyunAQBSlWVws3IaACBVWQY3u4MBAFKVZXBbXOMGAKQpy+DmGjcAIFVZBjfXuAEAqco0uLnGDQBI04yC2/Y82/VWV6Zd2B0MAJCqaQW37Zrt37f9z7Y3SLpX0hO219j+vO1j5raarcXKaQCAVE23x32jpKMkfUrSiyPisIhYJuk0STdJ+gvb581RHVuOa9wAgFQ1pvm5N0TE8N4nI2KjpKskXWW7o6U1m0Omxw0ASNS0gns8tG0fJOk9koYkrZF0V0TsnPyZFNRscZEbAJCiZienXSOpT9J/l/R5Sc/avrfltZpjXOMGAKRqukPl4xZExOdsvysiftv2v5F09FxUbC6xchoAIFXN9riHyuMu2z0RcZWkt7S4TnOuVrNGx6quBQAAzWu2x/0F20sl/ZOkv7P9M0nLW1+tuVWvMVQOAEhTUz3uiLgqIjZGxBclfVfSYZLeMSc1m0N1W6PcDwYASFCzPe4JEXFZKyvSTrUa17gBAGnKcq1yetwAgFRlGdzF5DSCGwCQnqaC2/aL93p9iO2u1lZp7tXNUDkAIE3N9ri/ttfryyTda/sLzfyI7cW2v2X7Xtv32H51k/WYlTo9bgBAopqanBYRZ+/1+g22LenYJsv9K0nXRcS7bXdK6m3y+7NSq1mj5DYAIEH77XHbPtb25ZNe/4vtEyZ/Jgprplug7YWSTlfZe4+I3RGxualaz1Ld0hg9bgBAgp5vqPz7kj496fXHJX3J9tdtHzLDMo+UNCjp67Zvt32J7Xl7f8j2+bYHbA8MDg7OsKipMVQOAEjV8wX3GyX9t/EXEXFbRLxe0rWSrrP9Gds9TZbZkHSypIsi4iRJ2yV9cu8PRcTFEdEfEf19fX1NFrF/BDcAIFX7De6IuCsi3jf5XHlN+z5JF0n6sKT7bb+/iTLXSVoXETeXr7+lIsjbpl6zRplVDgBIULO3g/1E0npJX1KxRvm/lXSGpFNsXzyd34iIJyU9Zvsl5akzJd3dTD1mq2ZzjRsAkKRmlzz9Y0lrIp7TXf2w7Xua+J0PS7qinFH+oKQPNFmPWaHHDQBIVbO3g63ez9tn7+e9vX/nDkn9zZTdSix5CgBIVcuWPI2IB1v1W3OtVmOoHACQplkFd8pLnjJUDgBI0Wx73DNa8rRqxSYjVdcCAIDmzXg/bmlWS55Wql4Tm4wAAJLU7O1gs17y9EDA5DQAQKqaHSpvxZKnlWNyGgAgVU0Fd4uWPK0ck9MAAKlqenJaC5Y8rRxrlQMAUtX2JU8PBLWamZwGAEhSVUueVorJaQCAVE2rx10OjysiVk8R2uOmveRp1Yoet7TvPwUAgAPTdIfKb7T9YdsrJ5+03Wn79bYvlXR666s3N+rFf4eITjcAIDXTHSo/S9K/k3Sl7SMkbZbULaku6XpJXyo3DklCvfzPldGxUL3maisDAEATphXcETEk6UJJF9rukHSwpJ0RsXkO6zZnarXxHjddbgBAWpq+HSwihiPiifHQtv3Tltdqjo0PlTNBDQCQmlZs63loC36jrcaHx1mEBQCQmmkNldv+a0l3lY/VEbF10tvJpd9EcI8mV3UAQOamOzntLkmvkPQ+Scfb3qI9Qb5gjuo2Z+hxAwBSNd3Jab+2KprtFSqC/OWSvjcH9ZpTtfHbwbjGDQBIzIz2446IdZLWSfpua6vTHvS4AQCpasXktOQwqxwAkKosg3viPu6xiisCAECTsgzuiZXTGCoHACSm2W09f9f2gvL5p21fbfvkuana3KkxVA4ASFSzPe7/GhFbbb9W0pskXSrpotZXa27VWfIUAJCoZoN7tDyeLemiiPi2pM7WVmnuMTkNAJCqZoN7ve2vSnqPpO/a7prBb1RufHIawQ0ASE2zofseFQuunFVuMrJE0n9pdaXm2p79uAluAEBamg3usyXdEBH32/60iq0+n259teZWnR43ACBRWU5OYz9uAECqMp+cVnFFAABoUqaT04ojQ+UAgNTMdnLaUjE5DQCAtmkquCNih6S1kt5k+z9KWhYR189JzeYQk9MAAKlqdsnTCyRdIWlZ+bjc9ofnomJzqca2ngCARDW7H/cHJf1WRGyXJNt/Kennkv661RWbSxND5fS4AQCJaTa4rT0zy1U+d7OF2n5Y0tby+yMR0d/sb8zG+FD5CMENAEhMs8H9dUk3276mfH2OpK/NsOzXRUQli7dMbDJCcAMAEtPs5LQvSvqApI2SNpXPk1PnGjcAIFHN9rgVEbdJum38te1vS/pysz8j6XrbIemrEXHx3h+wfb6k8yVp5cqVzVZzv9iPGwCQqlYsntL0NW5Jp0bEyZLeLOlDtk/f+wMRcXFE9EdEf19f36wrORn7cQMAUtWK4G46/SLi8fK4QdI1kk5pQT2mjSVPAQCpmtZQue2tmjqgLamnmQJtz5NUKzcrmSfpjZI+18xvzNb4kqdMTgMApGZawR0RC1pY5oskXeOi19uQ9I8RcV0Lf/95MTkNAJCqpienzVZEPCjphHaXO1mdyWkAgEQlt7NXK7AfNwAgVVkGNz1uAECqsgzuGruDAQASlWVwcx83ACBVeQY393EDABKVZXBP3MdNjxsAkJgsg3u8xz0ySnADANKSZ3BPTE5jrBwAkJYsg9u2Ous1DTOrHACQmCyDW5IadWt4hB43ACAt2QZ3R72mEXrcAIDEZBzc1m7uBwMAJCbj4K5phOAGACQm2+Bu1K1hbgcDACQm2+DuqNcYKgcAJCff4K4xVA4ASE++wd1gqBwAkJ5sg7tRq2mYHjcAIDHZBndnneAGAKQn2+Bu1M0mIwCA5GQb3B30uAEACco4uJmcBgBIT8bBTY8bAJCebIO7wSYjAIAEZRvcHXVrN9t6AgASk29w12oaGSO4AQBpyTe4WTkNAJCgfIObyWkAgAQR3AAAJCTj4GblNABAerIN7katuB0sgvAGAKQj2+DubBR/OhPUAAApyTa4GzVLEte5AQBJyTa4O+rFn851bgBASjIO7qLHvZseNwAgIRkHd9njZvU0AEBCKgtu23Xbt9u+toryG2VwD48wVA4ASEeVPe4LJN1TVeHjQ+XD9LgBAAmpJLhtr5B0tqRLqihf2jNUzqxyAEBKqupxf1nSxyXtMzVtn297wPbA4OBgyyvArHIAQIraHty23yppQ0Tcur/PRcTFEdEfEf19fX0tr0eDWeUAgARV0eM+VdLbbT8s6RuSXm/78nZXopMeNwAgQW0P7oj4VESsiIhVks6V9K8RcV6768HKaQCAFOV7H3eDyWkAgPQ0qiw8In4g6QdVlD0+VL57hOAGAKQj2x53d0fxpw8R3ACAhGQc3HVJ0tDu0YprAgDA9GUb3L2dxVWCncMENwAgHdkGd0/Z4ya4AQApyTa4u8pZ5TsZKgcAJCTb4K7VrO6OmobocQMAEpJtcEvFcDlD5QCAlBDcDJUDABKSdXB3d9LjBgCkJevg7umoc40bAJCU7IObHjcAICV5B3cn17gBAGnJOri7O+raOcxa5QCAdGQd3MWs8pGqqwEAwLRlHdy9zCoHACQm6+Du5j5uAEBisg7uns66hrjGDQBISN7B3VHX7tExjYwS3gCANGQf3JI0NEJwAwDSkHVwd3eWe3JznRsAkIisg3t+VxHc23ZxSxgAIA1ZB/fink5J0uYduyuuCQAA05N3cPd2SJI27xiuuCYAAExP5sFd9Lg30eMGACQi6+BeQo8bAJCYrIN7YXeHbK5xAwDSkXVw12rWop4Obd5JjxsAkIasg1uSlvR2ahND5QCARGQf3It6OhgqBwAkI/vgXtLbweQ0AEAysg/uxb2d3A4GAEhG9sF90LxOPb1tlyKi6qoAAPC8sg/uFUt6NDQ8pqe30esGABz4sg/ulQf1SpIe3bij4poAAPD8CO6lRXA/RnADABKQfXCvWEJwAwDS0fbgtt1t+xbbv7S9xvaftbsOk3V31LVsQRdD5QCAJDQqKHOXpNdHxDbbHZJ+Yvv/RcRNFdRFkrTqoHlaO7itquIBAJi2tve4ozCekh3lo9J7sU5auVir12/R0PBoldUAAOB5VXKN23bd9h2SNki6ISJurqIe4/pXLdXu0THdtf7ZKqsBAMDzqiS4I2I0Ik6UtELSKbaP3/szts+3PWB7YHBwcE7r88rDl0iSbn7wmTktBwCA2ap0VnlEbJb0A0lnTfHexRHRHxH9fX19c1qPpfM6ddLKxfr2HY+zghoA4IBWxazyPtuLy+c9kt4g6d5212Nv5/7mYbp/wzbd/NDGqqsCAMA+VdHjPkTSjbbvlPQLFde4r62gHr/mbSccqmULuvTn/3y3RkbHqq4OAABTqmJW+Z0RcVJEvCIijo+Iz7W7DlPp7Wzos28/TqvXb9FnvrOGIXMAwAEp+5XTJnvLyw/RH//2Ubri5kf1n775S24PAwAccKpYgOWA9omzXqIF3Q19/nv3afX6Z/Wl3ztRxy9fVHW1AACQRI/7OWzrQ687Wpd98BRtGRrWOy/8qf7Hdfdqx+6RqqsGAADBvS+nHdOn6y44XW874VBd+IO1+p0v/kjfW/Mk174BAJUiuPdjybxOffE9J+qbf/Rqze9q6I8uu1Xvu+Rm3fHY5qqrBgDIFME9DaccsVTXfuS1+uzbjtV9T27VOV/5qf7k8lvZmAQA0HZOYei3v78/BgYGqq6GJGnbrhH97Y8e1CU/flBDI2M658Tl+pMzjtTRyxZUXTUAwAuE7Vsjon/K9wjumXl62y5deONa/eMtj2jXyJjeeOyL9B/OOFonHLa46qoBABJHcM+hZ7bt0t//7GFd+rOHtWVoRKccsVR/8OrD9cZjX6zOBlciAADNI7jbYOvQsK685VFddtMjemzjTvUt6NJ7f/Mwvfe3VuqQRT1VVw8AkBCCu41Gx0I/+tWgLrvpEd143wZZ0qlHH6x3nrRcbzruxZrXxZo3AID9I7gr8tjGHfrmwGO65vb1Wrdpp3o76zrruBfrrSccotccdbC6O+pVVxEAcAAiuCs2NhYaeGSTrrl9na698wltHRrRvM66znjpMr3x2BfpdS9dpoXdHVVXEwBwgCC4DyC7Rkb1s7XP6Po1T+qGu5/S09t2q6Nu9R++VK895mCddszBOu7QRarXXHVVAQAVIbgPUKNjodsf3aTr735KP/rVoO59cqskaXFvh0496mC96qiD1H/4Ev3GixYQ5ACQEYI7EYNbd+mnDzytH9//tH7ywKCe2rJLkrSgq6ETVy7WKw9fov7Dl+rlyxdpUS9D6wDwQkVwJygi9NjGnbr10Y0aeHiTbn1kk+57aqvG/9+1fHGPjjt0oY47dJGOPXShjjt0oQ5Z1C2bnjkApG5/wc29SQco21p5UK9WHtSrd560QpK0ZWhYv3xss1av36I1jz+rux/fohvueWoizBd0NXTksvk6qm+ejuqbr6P65uvoZfO1cmkvi8EAwAsEwZ2Qhd0dOu2YPp12TN/Eue27RnTPE1t09xNb9MCGbVo7uE0/e+AZXX3b+onP1CwdsqhHy5f0aMWSHq1Y0qvDyuOKJT1atrBLXQ1uTQOAFBDciZvX1VD/qqXqX7X0185vHRrWQ09v19rBbXpocLvWbdqpdZt26qa1z+iJLeu19xWSxb0dWragS8sWdGvZwvK4oEvLFnZpaW+nFvd2asm8Di3p7eT+cwCoEMH9ArWgu0OvWLFYr1ix+Dnv7R4Z05PPDumxTTu0ftNOPbVlSBu27po4PvTgdm3YOqTh0annP3R31LRkPMx7izBf2NOhBd0NzetsaH53Q/O76prf1aF5XfXifFdD88tHT2ddnfUa1+MBYAYI7gx1NmoT18/3ZWwstHnnsDZsHdLG7bu1ecewNu0ojpt37NamScd7ntyiLTtHtH3XiHYOj06rDjVLXY26ujtq6umoq7ujrq6O4nV3eb67PN/dUVNXo66OutVRr6lRr6mjZnU0amrUrM5GTY1aTY261Vkvjh31mjrqVqNWm3herxWPmicf9WuvazWrbqtWU3EcP1eetzXxvMYtegAqQHBjSrWatXRep5bO62zqeyOjY9q+a1Tbdo9o29CItu0qHtt3Fa+37hrRzt0jGhoe09DwqIZGRvc8Hx7TrpFRDQ2P6ultI895f2Q0NDI2ts+RgCoUgV9MJrQkW7JcHvec1+TXe73n8gN7zj/3NzR+fjq/P/GbUyu/8dzzM/jvkH2Nmuzvp/ZVzj7P7+fX9v2d5iuwr+8wMITpOOWIpfrUm1/WlrIIbrRUo17Tot7anN5nHhEaGQsNjxYhPlIei9djz3lv9+iYRkZDo2PlI0IRodExaTRCY+X5sSge4+eLz0x+r1g0Z6z8jbGx8lz5PRX/p4hQTDyXQjExpyAinnN+/LXGX0/xXmiv3584v+e19ipr6rbbx3nt+0v7/k5zn9/ft5oto/jOPn5rv99prpwUbpfFgaGnjXN/CG4kx/bEsDkA5Ib/5QMAICEENwAACSG4AQBICMENAEBCCG4AABJCcAMAkBCCGwCAhBDcAAAkhOAGACAhBDcAAAkhuAEASAjBDQBAQghuAAAS4hS2rbM9KOmRFv7kwZKebuHv5Yg2bA3acfZow9mjDWev1W14eET0TfVGEsHdarYHIqK/6nqkjDZsDdpx9mjD2aMNZ6+dbchQOQAACSG4AQBISK7BfXHVFXgBoA1bg3acPdpw9mjD2WtbG2Z5jRsAgFTl2uMGACBJ2QW37bNs32f7AdufrLo+Byrbh9m+0fY9ttfYvqA8v9T2DbbvL49LJn3nU2W73mf7TdXV/sBhu277dtvXlq9pvybZXmz7W7bvLf95fDXt2BzbHyv/PV5t+0rb3bTh87P9d7Y32F496VzT7Wb7lbbvKt/7X7Y9m3plFdy265K+IunNko6V9F7bx1ZbqwPWiKT/HBEvk/QqSR8q2+qTkr4fEcdI+n75WuV750o6TtJZki4s2zt3F0i6Z9Jr2q95fyXpuoh4qaQTVLQn7ThNtpdL+oik/og4XlJdRRvRhs/v71W0wWQzabeLJJ0v6ZjysfdvNiWr4JZ0iqQHIuLBiNgt6RuS3lFxnQ5IEfFERNxWPt+q4n8sl6tor0vLj10q6Zzy+TskfSMidkXEQ5IeUNHe2bK9QtLZki6ZdJr2a4LthZJOl/Q1SYqI3RGxWbRjsxqSemw3JPVKely04fOKiB9J2rjX6abazfYhkhZGxM+jmFT2D5O+MyO5BfdySY9Ner2uPIf9sL1K0kmSbpb0ooh4QirCXdKy8mO07XN9WdLHJY1NOkf7NedISYOSvl5ecrjE9jzRjtMWEeslfUHSo5KekPRsRFwv2nCmmm235eXzvc/PWG7BPdV1BabV74ft+ZKukvTRiNiyv49OcS7btrX9VkkbIuLW6X5linPZtt8kDUknS7ooIk6StF3l0OQ+0I57Ka/BvkPSEZIOlTTP9nn7+8oU57Juw2naV7u1vD1zC+51kg6b9HqFiiEjTMF2h4rQviIiri5PP1UO/ag8bijP07a/7lRJb7f9sIpLMq+3fblov2atk7QuIm4uX39LRZDTjtP3BkkPRcRgRAxLulrSa0QbzlSz7baufL73+RnLLbh/IekY20fY7lQxkeA7FdfpgFTOevyapHsi4ouT3vqOpD8sn/+hpG9POn+u7S7bR6iYgHFLu+p7oImIT0XEiohYpeKfs3+NiPNE+zUlIp6U9Jjtl5SnzpR0t2jHZjwq6VW2e8t/r89UMWeFNpyZptqtHE7favtVZfv/waTvzExEZPWQ9BZJv5K0VtKfVl2fA/Uh6bUqhnPulHRH+XiLpINUzKS8vzwunfSdPy3b9T5Jb676bzhQHpLOkHRt+Zz2a779TpQ0UP6z+H8lLaEdm27DP5N0r6TVki6T1EUbTqvdrlQxL2BYRc/5gzNpN0n9ZduvlfS/VS5+NtMHK6cBAJCQ3IbKAQBIGsENAEBCCG4AABJCcAMAkBCCGwCAhBDcAJpm+4zxHc8AtBfBDQBAQghu4AXM9nm2b7F9h+2vlvuDb7P9P23fZvv7tvvKz55o+ybbd9q+ZnyfYdtH2/4X278sv3NU+fPzvWef7CvG9xi2/Re27y5/5wsV/enACxbBDbxA2X6ZpN+TdGpEnChpVNL7JM2TdFtEnCzph5I+U37lHyR9IiJeIemuSeevkPSViDhBxRrXT5TnT5L0URV72x8p6VTbSyW9U9Jx5e/8+Vz+jUCOCG7ghetMSa+U9Avbd5Svj1Sxzeg/lZ+5XNJrbS+StDgifliev1TS6bYXSFoeEddIUkQMRcSO8jO3RMS6iBhTsSTuKklbJA1JusT2uySNfxZAixDcwAuXJV0aESeWj5dExGen+Nz+1j2eakvCcbsmPR+V1IiIEUmnqNhV7hxJ1zVXZQDPh+AGXri+L+ndtpdJku2ltg9X8e/9u8vP/L6kn0TEs5I22T6tPP9+ST+MYg/2dbbPKX+jy3bvvgos929fFBHfVTGMfmLL/yogc42qKwBgbkTE3bY/Lel62zUVOxx9SNJ2ScfZvlXSsyqug0vFFoV/Uwbzg5I+UJ5/v6Sv2v5c+Ru/u59iF0j6tu1uFb31j7X4zwKyx+5gQGZsb4uI+VXXA8DMMFQOAEBC6HEDAJAQetwAACSE4AYAICEENwAACSG4AQBICMENAEBCCG4AABLy/wHAGkQGNBU3+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Loss $L(y,\\hat{y};a)$')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as close as we can make the model $f$ predict $y$ from $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7782, 2.4997, 1.9390, 5.2707], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 2., 5.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We looked at the flow to create a custom Neural network.\n",
    "- The prediction don't matter here because we don't have any data reasonable for a neural network model.\n",
    "- Here we have a very basic NeuralNet with 24 parameters, first layer of 16 and second layer of 8.\n",
    "- We will build complex NeuralNets in future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Pytorch_Mr-P-Solver')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd333f409411bc5446bc89fd81b942417c0fcf2aa15c46865d6096bbe4424b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
